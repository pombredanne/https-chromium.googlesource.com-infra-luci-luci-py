{
  "comments": [
    {
      "unresolved": false,
      "key": {
        "uuid": "59a60076_38db0808",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 2
      },
      "lineNbr": 0,
      "author": {
        "id": 1547372
      },
      "writtenOn": "2023-11-10T23:55:58Z",
      "side": 1,
      "message": "Would like some early feedback on this approach before I spend more time fixing tests and doing cleanup. \n\nMy first attempt at the linked bug was to find a faster way to write `get_recursive_size`. I think `os.scandir` is actually quite fast already, at least on `linux`. I tested it against `du` and it was actually faster. Have not tested on `macos` yet. \n\nIt occurred to me that we probably don\u0027t need to actually calculate the sizes of the `named_cache` until after the task is complete. IUUC its not used for anything useful besides logging.\n\nEven if some caches show `zero` for their file size, this probably won\u0027t effect the name cache hints bc https://source.chromium.org/chromium/infra/infra/+/main:luci/appengine/swarming/server/named_caches.py;l\u003d208\n\nTrimming function which runs before execution doesn\u0027t even rely on the actual sizes of the `NamedCache`. Only the `hints` which it gets from the server. \n\nSo I don\u0027t see why we need to calculate `cache size` during a task, specially if it\u0027s expensive.",
      "revId": "c237de85ba230893660ce24cce941f41659e425c",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "fcc4d359_78c49e61",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 2
      },
      "lineNbr": 0,
      "author": {
        "id": 1002539
      },
      "writtenOn": "2023-11-11T00:45:27Z",
      "side": 1,
      "message": "I think this can have negative consequences, worse than the current state.\n\nCache cleaning happens between tasks. This is currently totally \"invisible\" activity that doesn\u0027t show up any event logs, BQ, metrics, etc. From monitoring graphs it will look like bots sit idle for no apparent reason. \n\nEven worse, during the cleaning the bot is not sending any keep-alive pings to Swarming or RBE. If it takes too long, the bot will be considered dead. If this is last bot of its class, this will result in all tasks pending for this bot class dropped with \"no resources\" error (this has happened for real in a similar scenario where a bot hook takes too long).\n\nConsidering we are in a middle of rewrite and don\u0027t have capacity to do big changes (like introduction of a special maintenance state that shows up in event logs and monitoring), I think we should just try to optimize the cache size evaluation. Or maybe get rid of it completely (e.g. do it only when there\u0027s a disk pressure and we need to evict some caches).",
      "revId": "c237de85ba230893660ce24cce941f41659e425c",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "21af4842_bb4f7761",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 2
      },
      "lineNbr": 0,
      "author": {
        "id": 1547372
      },
      "writtenOn": "2023-11-14T17:20:02Z",
      "side": 1,
      "message": "That makes a lot of sense! I think I assumed too much about how the `clean` function works then! \n\nWrt optimizations, we could possibly expect a 2x speedup in some cases. I did the following experiment on `macos`.\n\n1. Generated a 6 level deep directory structure with ~\u003d 800k 1byte files.\n2. Compared a series of tools to measure their performance against the current python implementation.\n3. Calculated the average wallclock time of the tools execution.\n\nBut first some notes about my investigation into this. \n\nWe cannot directly use `du` because it is calculating `disk space` while the `get_recursive_size` is calculating the sum of all file sizes. \nSo they are not doing the same thing. In hindsight, I think `du` may have been a more realistic to use. To get `du` to produce the same result as `get_recursive_size`, it becomes non cross platform and actually a bit slower. \n\nThat said, I found a rust `cli` tool (https://github.com/solidiquis/erdtree) which does a `logical` disk size calculation which is the same as what we want. I\u0027m not saying we use this specific tool but wanted to use it as a `baseline` to see what sort of performance we could hope for. \n\nRunning (avg time 20 runs `5.71415`): \n```\nerd --level 0 --no-ignore --disk-usage logical test_dir_1\n\n817645 B test_dir_1\n13606 directories, 817645 files\n```\n\nAvg for `get_recursive_file_size` was `11.9888`. \n\nWe can conclude from this that we may see a `2x` or so speed up on `macos` for similar directory structures I tested if we heavily optimize the `get_recursive_file_size` routine. \n\nIf the results of the experiment are generalizable, then we could, with some sort of optimization shave off ~\u003d 150seconds of time. But I think this assertion is highly speculative. \n\nOther caveats, I did not measure cpu or memory usage. But could look into that in the future.",
      "parentUuid": "fcc4d359_78c49e61",
      "revId": "c237de85ba230893660ce24cce941f41659e425c",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "9c32d203_b780a90e",
        "filename": "client/utils/file_path.py",
        "patchSetId": 2
      },
      "lineNbr": 1033,
      "author": {
        "id": 1002539
      },
      "writtenOn": "2023-11-11T00:45:27Z",
      "side": 1,
      "message": "This may be as slow as get_recursive_size itself, since scandir actually returns file sizes. This only difference between is_empty_dir_recursive and _get_recur_size_with_scandir is absence of some summing, which is likely very minor part of _get_recur_size_with_scandir runtime.",
      "range": {
        "startLine": 1033,
        "startChar": 4,
        "endLine": 1033,
        "endChar": 26
      },
      "revId": "c237de85ba230893660ce24cce941f41659e425c",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba"
    }
  ]
}